# -*- coding: utf-8 -*-
"""AI_Based_Energy_Consumption_Prediction_and_Optimization_for_DLC_GPUs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HgvtXVPUyP9AK1KUYCOdAm7ON-8Kx59V

# AI-Based Energy Consumption Prediction & Optimization for DLC GPUs

### Project by: Muhammad Hamza

#### Dated: 21 Feb, 2025

#### üåê Website: https://mhamza.site/
#### üì© Contact: https://mhamza.site/contact/

### 1. Introduction

Direct Liquid Cooling (DLC) is a cutting-edge cooling method used to manage the heat generated by high-performance GPUs. Optimizing cooling strategies is critical for reducing energy consumption, operational costs, and hardware degradation. Traditional cooling systems operate inefficiently by running at fixed rates, consuming unnecessary power during low workloads and failing to cool effectively during high-demand periods.

Artificial Intelligence (AI) provides a predictive and adaptive approach to cooling by forecasting energy consumption and dynamically adjusting cooling parameters. This report explores AI-based energy forecasting for DLC GPUs, compares different machine learning (ML) models, and presents a working Python implementation.

### 2. Problem Statement

#### Why Predict GPU Energy Consumption?
	‚Ä¢	Cooling costs contribute significantly to the total energy consumption in high-performance computing.
	‚Ä¢	Inefficient cooling increases power usage and reduces hardware lifespan.
	‚Ä¢	Predicting power consumption allows us to adjust cooling dynamically,
             preventing overheating and reducing operational costs.

### Key Objectives
	1.	Predict GPU power consumption based on historical data and environmental factors.
	2.	Optimize cooling system response dynamically.
	3.	Reduce energy costs by preventing unnecessary cooling power usage.
	4.	Extend hardware lifespan by minimizing thermal stress.

## 3. Background Concepts

### 3.1 Energy Consumption & Cooling Efficiency

#### GPU power usage is affected by:
	‚Ä¢	Workload (AI computations, gaming, rendering).
	‚Ä¢	Ambient temperature (higher temperatures lead to increased power draw).
	‚Ä¢	Cooling system efficiency (poor cooling leads to overheating and throttling).
	‚Ä¢	Humidity & airflow conditions.

### 3.2 Time-Series Forecasting

##### Predicting energy consumption is a time-series problem, as power usage varies over time. Common models used:
	‚Ä¢	ARIMA (traditional statistical model).
	‚Ä¢	Random Forest/XGBoost (tree-based ML models for structured data).
	‚Ä¢	LSTMs & Transformers (deep learning models for sequential data).

#### 3.3 Dynamic Cooling Control

##### AI can adjust cooling power dynamically based on energy predictions by:
	‚Ä¢	Increasing cooling before GPU overheats.
	‚Ä¢	Reducing cooling when power usage is low.
	‚Ä¢	Avoiding unnecessary cooling costs.

### 4. AI Solution Approach

#### 4.1 Data Collection

#### Two primary datasets:
	1.	GPU Power Consumption Logs ‚Äì Collected using tools like nvidia-smi or cloud-based monitoring.
   
	2.	Weather Data ‚Äì Fetched from free APIs like OpenWeatherMap and Meteostat.

    https://openweathermap.org/api
    https://dev.meteostat.net/

#### 4.2 Feature Engineering
	‚Ä¢	Time Features: Hour of the day, day of the week.
	‚Ä¢	Weather Features: Temperature, humidity, wind speed.
	‚Ä¢	GPU Metrics: Utilization, workload type.

#### 4.3 Model Selection

We compare three ML approaches:

![image.png](attachment:e657e209-8b70-4ab5-ac2f-62b06f781343.png)

#### 5. Python Implementation

5.1 Install Required Libraries
"""

print ("hello")

import pandas, numpy, sklearn, xgboost, requests, matplotlib
print("All libraries are installed!")

import sys
print(sys.path)

sys.path.append('/lib/python3.12/site-packages')  # Adjust for your Python version

# Commented out IPython magic to ensure Python compatibility.
# %pip install meteostat

# üìå Step 1: Import Required Libraries
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error
import matplotlib.pyplot as plt

print("All libraries are imported!")

"""#### I have generated a synthetic dataset for GPU energy consumption and weather conditions over a 10-day period with hourly data. The dataset includes:
	‚Ä¢	Temperature (¬∞C)
	‚Ä¢	Humidity (%)
	‚Ä¢	Wind Speed (km/h)
	‚Ä¢	GPU Power Consumption (Watts)

#### We can now use this dataset to train machine learning models for energy prediction and cooling optimization.

# Step 2: Generate Synthetic Weather and GPU Energy Data
"""

# Generate timestamps for 10 days (hourly data)
timestamps = pd.date_range(start="2025-01-01", periods=240, freq='h')

print("timestamps: ", timestamps)

# Generate synthetic weather data
temperature = np.random.uniform(0, 35, len(timestamps))  # Temperature in ¬∞C
humidity = np.random.uniform(20, 80, len(timestamps))  # Humidity in %
wind_speed = np.random.uniform(0, 20, len(timestamps))  # Wind Speed in km/h

print("temperature ¬∞C: ", temperature)

print("humidity %: ", humidity)

print("wind_speed km/h: ", wind_speed)

# Generate synthetic GPU power consumption (in watts)
gpu_energy = np.sin(np.linspace(0, 10, len(timestamps))) * 50 + 200 + np.random.uniform(-10, 10, len(timestamps))

print("gpu_energy W: ", gpu_energy)

# Create DataFrame
dummy_data = pd.DataFrame({
    'timestamp': timestamps,
    'temperature': temperature,
    'humidity': humidity,
    'wind_speed': wind_speed,
    'gpu_energy': gpu_energy
})

# Set timestamp as index
dummy_data.set_index('timestamp', inplace=True)

# Display the first few rows of the dataset
print("Synthetic Weather and GPU Energy Data (First 5 Rows):")
print(dummy_data.head())

"""# Step 3: Split Data for Machine Learning Models"""

# Selecting features and target variable
features = ['temperature', 'humidity', 'wind_speed']
X = dummy_data[features]
y = dummy_data['gpu_energy']

# Splitting dataset into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""# Step 4: Prepare Machine Learning Models"""

# Uncomment this section when you are ready to train the models
# Define machine learning models
models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(n_estimators=50, random_state=42),  # Reduced estimators for faster execution
    "XGBoost": XGBRegressor(n_estimators=10, random_state=42)  # Reduced boosting rounds for efficiency
}

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Train models and evaluate performance
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)  # Train the model
    y_pred = model.predict(X_test)  # Make predictions
    mae = mean_absolute_error(y_test, y_pred)  # Compute Mean Absolute Error (MAE)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))  # Compute RMSE
    r2 = r2_score(y_test, y_pred)  # Compute R¬≤ Score
    results[name] = {"MAE": mae, "RMSE": rmse, "R2 Score": r2, "Predictions": y_pred}

# Convert results into a DataFrame
performance_df = pd.DataFrame([{**{"Model": k}, **v} for k, v in results.items()]).drop(columns=["Predictions"])

print("performance_df: ", performance_df)

"""# Step 5: Visualize Model Performance"""

# Bar Chart for Mean Absolute Error (MAE)
plt.figure(figsize=(10, 5))
plt.bar(performance_df["Model"], performance_df["MAE"], color=['blue', 'green', 'red'])
plt.xlabel("Machine Learning Models")
plt.ylabel("Mean Absolute Error (Watts)")
plt.title("Model Performance Comparison (Lower MAE is Better)")
plt.grid(axis='y')
# Save the figure in the root directory
plt.savefig("bar_chart_for_mean_absolute_error.png")

#plt.show()
# Close the figure to prevent it from blocking execution
#plt.close("all")

# Print confirmation message
print("Plot saved as 'bar_chart_for_mean_absolute_error.png' in the root folder.")

"""### Conclusion:

#### Linear Regression (BEST)
	‚Ä¢	MAE: 28.86 | RMSE: 32.99 | R¬≤ Score: 0.0169
	‚Ä¢	Assumes a linear relationship between power consumption and weather variables.
	‚Ä¢	Struggles to capture complex dependencies, leading to higher errors.
	‚Ä¢	Works best as a baseline model, but lacks real-world applicability.

#### Random Forest
	‚Ä¢	MAE: 30.00 | RMSE: 35.47 | R¬≤ Score: -0.1365
	‚Ä¢	Handles non-linearity better than Linear Regression.
	‚Ä¢	Expected to perform better, but higher errors indicate possible overfitting.
	‚Ä¢	Not optimized for time-series dependencies, limiting its forecasting accuracy.
    
#### XGBoost:
	‚Ä¢	MAE: 29.43 | RMSE: 34.69 | R¬≤ Score: -0.0872
	‚Ä¢	Uses gradient boosting for iterative improvements.
	‚Ä¢	Expected to perform well, but underperformed due to hyperparameter limitations.
	‚Ä¢	Requires parameter tuning and a larger dataset for better generalization.
"""

# Display Model Performance Metrics
print("\nüìå Model Performance Metrics:")
print(performance_df)
print("I am here 3")

"""### Key Takeaways:

#### ‚úÖ Linear Regression performed the best in terms of R¬≤ Score (0.0169), but still had relatively high errors.
#### ‚úÖ Random Forest surprisingly underperformed, with the highest MAE (30.00) and RMSE (35.47), suggesting overfitting.
#### ‚úÖ XGBoost had negative R¬≤ (-0.0872), meaning it did worse than a simple average prediction, requiring hyperparameter tuning.


### Observations from the Table:
#### Mean Absolute Error (MAE) ‚Üí Measures average absolute differences between actual and predicted values.
	‚Ä¢	Lowest: Linear Regression (28.86) ‚Üí Performs slightly better in absolute error reduction.
	‚Ä¢	Highest: Random Forest (30.00) ‚Üí Indicates higher prediction inconsistencies.
#### Root Mean Squared Error (RMSE) ‚Üí Penalizes large errors more than MAE.
	‚Ä¢	Lowest: Linear Regression (32.99) ‚Üí Handles large fluctuations better.
	‚Ä¢	Highest: Random Forest (35.47) ‚Üí Suggests larger prediction errors.
#### R¬≤ Score (Coefficient of Determination) ‚Üí Measures how well the model explains variability (closer to 1 is better).
	‚Ä¢	Highest: Linear Regression (0.0169) ‚Üí Explains very little variance but still better than others.
	‚Ä¢	Lowest: XGBoost (-0.0872) ‚Üí Negative value means the model performs worse than a basic mean predictor.

### How to Improve Model Performance?

#### ‚úÖ Feature Engineering
	‚Ä¢	Introduce time-based features (hour of the day, weekday vs. weekend).
	‚Ä¢	Incorporate GPU workload/utilization metrics to improve accuracy.

#### ‚úÖ Hyperparameter Tuning
	‚Ä¢	Optimize XGBoost parameters (learning rate, estimators, max depth).
	‚Ä¢	Perform Grid Search or Bayesian Optimization for fine-tuning.

#### ‚úÖ Switch to Time-Series Models
	‚Ä¢	Instead of Random Forest and XGBoost, try LSTM or Transformer models to better capture sequential energy consumption patterns.

#### ‚úÖ Use Real-World Data
	‚Ä¢	Fetch live weather data from OpenWeatherMap API to improve prediction accuracy.
	‚Ä¢	Train on a larger dataset to help models generalize better.

# Step 6: Visualizing the Data
"""

# Plot GPU energy consumption over time
plt.figure(figsize=(12, 5))
plt.plot(dummy_data.index, dummy_data['gpu_energy'], label="GPU Energy Consumption (Watts)", color='blue')
plt.xlabel("Timestamp")
plt.ylabel("Power Consumption (Watts)")
plt.title("Synthetic GPU Energy Consumption Over Time")
plt.legend()
plt.grid()
#plt.show()
#plt.show(block=False)

"""# Step 7: Future Predictions (Using Best Model)"""

# Generate synthetic future weather data for prediction
future_timestamps = pd.date_range(start="2025-03-01", periods=24, freq='h')
future_weather = pd.DataFrame({
    "temperature": np.random.uniform(0, 35, len(future_timestamps)),
    "humidity": np.random.uniform(20, 80, len(future_timestamps)),
    "wind_speed": np.random.uniform(0, 20, len(future_timestamps))
}, index=future_timestamps)

future_weather.head()

# Predict future GPU power consumption using the best model
# best_model = models["Random Forest"]
best_model = models["Linear Regression"]
future_predictions = best_model.predict(future_weather)

# Create DataFrame for future predictions
future_df = pd.DataFrame({"timestamp": future_timestamps, "Predicted GPU Energy": future_predictions})
future_df.set_index("timestamp", inplace=True)

"""# Step 8: Visualize Future Predictions"""

# Step 8: Visualize Future Predictions

plt.figure(figsize=(12, 5))
plt.plot(future_df.index, future_df["Predicted GPU Energy"], label="Predicted GPU Energy (Watts)", color='purple', linestyle='--')
plt.xlabel("Timestamp")
plt.ylabel("Predicted Power Consumption (Watts)")
plt.title("Future Predictions of GPU Energy Consumption")
plt.legend()
plt.grid()
plt.show()

# Display Future Predictions
print("\nüìå Future GPU Energy Predictions:")
print(future_df)

"""# Future Integration with Real Data
This implementation currently uses **synthetic data**, but real-time weather data can be used by fetching it from APIs such as:
- OpenWeatherMap: https://openweathermap.org/api
- Meteostat: https://dev.meteostat.net/

By replacing the dummy dataset with real API-fetched weather data, we can enhance the model's predictive accuracy in practical applications.

#### Features of This Code

##### ‚úÖ Generates synthetic weather & GPU power data.
##### ‚úÖ Trains multiple machine learning models (Linear Regression, Random Forest, XGBoost).
##### ‚úÖ Compares performance using MAE, RMSE, and R¬≤ Score.
##### ‚úÖ Displays model performance in a bar chart.
##### ‚úÖ Predicts and visualizes GPU energy consumption for the next 24 hours.
##### ‚úÖ Future integration possible with real-time data APIs.

### 7. Implementation of AI-Based Cooling

#### 7.1 How to Apply AI in Cooling Optimization?

##### ‚úÖ If AI predicts high energy consumption: Increase cooling preemptively.
##### ‚úÖ If AI predicts low energy consumption: Reduce cooling power to save costs.
##### ‚úÖ If peak pricing is detected: Schedule heavy workloads for off-peak hours.

#### 7.2 Integrating AI with Cooling Systems
	‚Ä¢	Connect AI to fan controllers and liquid cooling pumps.
	‚Ä¢	Use Reinforcement Learning (RL) to optimize cooling dynamically.
	‚Ä¢	Deploy AI model in real-time monitoring systems.

### 8. Conclusion

##### ‚úÖ AI-powered energy forecasting reduces cooling costs.
##### ‚úÖ Machine learning models (XGBoost) outperform simple models.
##### ‚úÖ Predictive cooling extends GPU lifespan and avoids peak pricing.
##### ‚úÖ Future improvements: Deep Learning (LSTMs), cloud deployment, real-world integration.

### 9. Next Steps
	‚Ä¢	Collect real-world GPU logs to replace synthetic data.
	‚Ä¢	Implement LSTM models for long-term forecasting.
	‚Ä¢	Deploy model as an API to control cooling in real-time.

##**Testing LSTM Model**
24-Feb-2025
"""

# üìå Step 1: Import Required Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.model_selection import train_test_split

# üìå Step 2: Generate Synthetic Data (Realistic Variations & Anomalies)
np.random.seed(42)

timestamps = pd.date_range(start="2024-01-01", periods=1440, freq='h')

print("timestamps: ", timestamps)

# Simulated weather patterns
temperature = 15 + 10 * np.sin(np.linspace(0, 6.28, len(timestamps))) + np.random.normal(0, 2, len(timestamps))
humidity = 50 + 10 * np.cos(np.linspace(0, 6.28, len(timestamps))) + np.random.normal(0, 5, len(timestamps))
wind_speed = np.random.uniform(5, 25, len(timestamps))
air_pressure = np.random.uniform(990, 1025, len(timestamps))
dew_point = temperature - np.random.uniform(2, 5, len(timestamps))
cloud_cover = np.abs(50 * np.sin(np.linspace(0, 6.28, len(timestamps)))) + np.random.normal(0, 10, len(timestamps))

# GPU-related features
gpu_utilization = np.clip(40 + 30 * np.sin(np.linspace(0, 12.56, len(timestamps))) + np.random.normal(0, 10, len(timestamps)), 0, 100)
fan_speed = np.clip(2000 + 500 * np.sin(np.linspace(0, 12.56, len(timestamps))) + np.random.normal(0, 200, len(timestamps)), 1000, 4000)
core_temperature = np.clip(50 + 10 * np.sin(np.linspace(0, 12.56, len(timestamps))) + np.random.normal(0, 5, len(timestamps)), 30, 90)
vram_utilization = np.clip(4000 + 1000 * np.sin(np.linspace(0, 6.28, len(timestamps))) + np.random.normal(0, 500, len(timestamps)), 1000, 8000)
power_limit = np.random.uniform(80, 120, len(timestamps))

# Workload Types (Categorical Encoding)
workload_encoded = np.random.choice([0, 1, 2, 3, 4], len(timestamps), p=[0.3, 0.3, 0.2, 0.1, 0.1])

# Time-based features
hour_of_day = timestamps.hour
day_of_week = timestamps.dayofweek
month_of_year = timestamps.month

# Simulating GPU Energy Consumption (Target Variable)
gpu_energy = ((150 + (gpu_utilization * 1.8) + (temperature * 0.6) - (fan_speed * 0.002)) + ((core_temperature * 0.9) + np.random.normal(0, 5, len(timestamps))))

# Injecting Anomalies Every ~200 Hours
for i in range(0, len(gpu_energy), 200):
    gpu_energy[i] += np.random.uniform(50, 100)

# Create DataFrame
df = pd.DataFrame({
    'timestamp': timestamps,
    'temperature': temperature,
    'humidity': humidity,
    'wind_speed': wind_speed,
    'air_pressure': air_pressure,
    'dew_point': dew_point,
    'cloud_cover': cloud_cover,
    'gpu_utilization': gpu_utilization,
    'fan_speed': fan_speed,
    'core_temperature': core_temperature,
    'vram_utilization': vram_utilization,
    'power_limit': power_limit,
    'workload_type': workload_encoded,
    'hour_of_day': hour_of_day,
    'day_of_week': day_of_week,
    'month_of_year': month_of_year,
    'gpu_energy': gpu_energy
})

df.set_index('timestamp', inplace=True)

print("df: ", df.head())

# üìå Step 3: Preprocess Data (Normalization & Sequence Formatting)
features = df.columns[:-1]
target = 'gpu_energy'

scaler_features = MinMaxScaler()
scaler_target = MinMaxScaler()

df_scaled = df.copy()
df_scaled[features] = scaler_features.fit_transform(df[features])
df_scaled[target] = scaler_target.fit_transform(df[[target]])

data_array = df_scaled[features.tolist() + [target]].values
# Convert features to a list before concatenation

SEQ_LENGTH = 24  # Use last 24 hours to predict next-hour GPU energy

def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i+seq_length, :-1])
        y.append(data[i+seq_length, -1])
    return np.array(X), np.array(y)

X, y = create_sequences(data_array, SEQ_LENGTH)

# üìå Step 4: Split Data for Training & Testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)

# üìå Step 5: Build & Train LSTM Model
model = Sequential([
    LSTM(64, return_sequences=True, input_shape=(SEQ_LENGTH, X.shape[2])),
    Dropout(0.2),
    LSTM(64, return_sequences=False),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1)
])

model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data=(X_test, y_test), verbose=1)

# üìå Step 6: Evaluate Model Performance
y_pred = model.predict(X_test)

# Inverse scaling
y_pred_unscaled = scaler_target.inverse_transform(y_pred.reshape(-1, 1)).flatten()
y_test_unscaled = scaler_target.inverse_transform(y_test.reshape(-1, 1)).flatten()

# Performance Metrics
mae = mean_absolute_error(y_test_unscaled, y_pred_unscaled)
rmse = np.sqrt(mean_squared_error(y_test_unscaled, y_pred_unscaled))
r2 = r2_score(y_test_unscaled, y_pred_unscaled)

print(f"üìå LSTM Model Performance:")
print(f"MAE: {mae:.2f} Watts")
print(f"RMSE: {rmse:.2f} Watts")
print(f"R¬≤ Score: {r2:.4f}")

errors = np.abs(y_test_unscaled - y_pred_unscaled)
threshold = np.percentile(errors, 95)
anomalies = errors > threshold

anomaly_df = pd.DataFrame({
    "Timestamp": df.index[-len(y_test_unscaled):],
    "Actual GPU Energy": y_test_unscaled,
    "Predicted GPU Energy": y_pred_unscaled,
    "Error": errors,
    "Anomaly": anomalies
})

print("\nüìå Detected Anomalies in GPU Energy Consumption:")
print(anomaly_df[anomaly_df["Anomaly"] == True])

# üìå Step 8: Visualization
plt.figure(figsize=(12, 5))
plt.plot(df.index[-len(y_test_unscaled):], y_test_unscaled, label="Actual GPU Energy", color='blue')
plt.plot(df.index[-len(y_test_unscaled):], y_pred_unscaled, label="Predicted GPU Energy", color='red', linestyle='dashed')
plt.scatter(anomaly_df["Timestamp"][anomaly_df["Anomaly"]], anomaly_df["Actual GPU Energy"][anomaly_df["Anomaly"]], color='orange', label="Detected Anomalies", marker='o')
plt.xlabel("Timestamp")
plt.ylabel("GPU Energy Consumption (Watts)")
plt.title("LSTM Model: GPU Energy Consumption Prediction & Anomalies")
plt.legend()
plt.grid()
plt.show()

"""##**Designing a Scalable AI-Driven GPU Monitoring & Cooling System for Global Data Centers**

We have a scenario, where we have large-scale server farms with thousands of high-performance GPUs spread across different climate zones (hot like Saudi Arabia and cold like Canada). The goal is to monitor GPU performance, predict power consumption, detect anomalies, and optimize cooling using AI while centrally managing global operations.

#### 1Ô∏è‚É£ What Kind of GPUs Are Used in Large Data Centers?

In large server farms, GPUs are typically designed for AI, high-performance computing (HPC), and cloud rendering. These are rack-mounted, headless GPUs, optimized for efficiency & scalability.

#### ‚úÖ Common Data Center GPUs:
	1.	NVIDIA A100, H100 ‚Üí AI training, deep learning, and cloud inferencing.
	2.	NVIDIA RTX 6000/8000 ‚Üí Cloud-based rendering, scientific computing.
	3.	AMD Instinct MI250X, MI300 ‚Üí HPC & AI applications.
	4.	Google Cloud TPU (Tensor Processing Units) ‚Üí Specialized for AI workloads.
	5.	Intel Habana Gaudi2 ‚Üí Optimized for AI inferencing & training.

##### üîπ These GPUs operate inside rack-mounted blade servers in rows & columns with liquid cooling systems attached.

#### 2Ô∏è‚É£ What Kind of Data Do These GPUs Generate?

Large-scale GPU farms produce telemetry & operational data in real time.

#### ‚úÖ Common Metrics Sent by GPUs:
	‚Ä¢	Power Consumption (Watts)
	‚Ä¢	Utilization (%) ‚Üí GPU, Memory, VRAM, CUDA Cores.
	‚Ä¢	Temperature (¬∞C) ‚Üí GPU Core, Memory, VRMs.
	‚Ä¢	Clock Speeds (MHz) ‚Üí Core, Memory, Tensor Cores.
	‚Ä¢	Fan Speed (RPM) ‚Üí Only if not liquid-cooled.
	‚Ä¢	Error Logs (ECC, CRC) ‚Üí Memory & computation errors.
	‚Ä¢	Workload Type (AI, Rendering, HPC, Idle, etc.)

  #### üîπ How Do We Collect This Data?
	‚Ä¢	NVIDIA System Management Interface (nvidia-smi) ‚Üí CLI tool to extract GPU data.
	‚Ä¢	Prometheus + Node Exporter ‚Üí Collects and sends GPU telemetry.
	‚Ä¢	SNMP (Simple Network Management Protocol) ‚Üí Used in enterprise monitoring.
	‚Ä¢	IPMI (Intelligent Platform Management Interface) ‚Üí Extracts low-level power/thermal data.
	‚Ä¢	CUDA APIs & NVML (NVIDIA Management Library) ‚Üí Access to power, clocks, and usage data.

  #### üîπ Data Collection Rate?
	‚Ä¢	Every 1-5 seconds for real-time monitoring.
	‚Ä¢	Every 30-60 seconds for trend analysis.

#### 3Ô∏è‚É£ How Do We Extract Data from GPUs & Send It to a Central Server?

To collect, process, and send GPU telemetry globally, we use edge computing at each regional data center before forwarding it to a central AI-powered monitoring hub.

#### ‚úÖ Hardware Used for Data Collection:
	1.	Embedded Telemetry Devices (Raspberry Pi, Jetson Nano, or custom ARM-based boards).
	2.	Edge Servers (Mini HPC nodes inside each data center).
	3.	Direct GPU APIs (Using nvidia-smi, Prometheus exporters, or CUDA SDKs).

#### üîπ How Data Flows in the System?
	1.	Each GPU logs power, temperature, & usage ‚Üí Sends data to local Edge Server.
	2.	Edge Server preprocesses data (removes noise, filters anomalies).
	3.	Data is securely transmitted using MQTT, WebSockets, or gRPC to a regional processing hub.
	4.	AI Model on Cloud AI Hub processes incoming GPU telemetry for predictions.

#### ‚úÖ Best Transport Protocols for Real-Time GPU Data Transfer
	‚Ä¢	Kafka Streams ‚Üí High-throughput, fault-tolerant.
	‚Ä¢	MQTT (Message Queue Telemetry Transport) ‚Üí Low-latency, lightweight for IoT.
	‚Ä¢	WebSockets / gRPC ‚Üí Faster than HTTP, supports bi-directional communication.
	‚Ä¢	HTTP REST API ‚Üí Used for periodic reporting, but not for real-time telemetry.

#### 4Ô∏è‚É£ How Are DLC (Direct Liquid Cooling) Systems Connected to the GPUs?

Large-scale GPUs use liquid cooling instead of fans to dissipate heat efficiently in hot regions (e.g., Saudi Arabia, UAE, India).

#### ‚úÖ How DLC Works in a Data Center?
	1.	Each GPU has a coolant block (like water cooling on gaming PCs).
	2.	Coolant circulates using pumps ‚Üí Transfers heat away from GPUs.
	3.	Liquid absorbs heat ‚Üí Moves to external radiators or heat exchangers.
	4.	Heat is dissipated outside using industrial chillers.
	5.	Cooled liquid returns to GPUs to continue the cycle.

#### ‚úÖ How Do We Monitor Liquid Cooling Performance?
	‚Ä¢	Flow Rate (L/min) ‚Üí Monitors coolant speed.
	‚Ä¢	Coolant Temperature In/Out (¬∞C) ‚Üí Determines cooling efficiency.
	‚Ä¢	Pump Speed (RPM) ‚Üí Ensures proper circulation.
	‚Ä¢	Leak Detection Sensors ‚Üí Prevents failures & coolant loss.

##### üîπ These sensors send telemetry to the Edge Servers ‚Üí The AI model detects failures & predicts overheating.

#### 5Ô∏è‚É£ Where Will We Host the Machine Learning Models?

Since data centers are globally distributed, we need a centralized AI hub to manage predictions.

#### ‚úÖ Best Hosting Options for AI-Based Energy Forecasting Models:
	1.	Google Cloud AI / AWS Sagemaker ‚Üí Cloud-hosted model inference.
	2.	On-Prem Kubernetes Cluster ‚Üí Self-managed AI workloads inside data centers.
	3.	Edge AI with NVIDIA Jetson Servers ‚Üí Fast AI inference at each local region.

#### üîπ Best Architecture for AI Inference?
	‚Ä¢	Step 1: GPUs send telemetry to Regional Edge AI Nodes for real-time monitoring.
	‚Ä¢	Step 2: AI models at the Central AI Hub process the data for predictive maintenance & energy forecasting.
	‚Ä¢	Step 3: Predictions & alerts are sent back to regional data centers for automated cooling control.

#### ‚úÖ Best Database for Storing Global GPU Data?
	‚Ä¢	Apache Cassandra / ScyllaDB ‚Üí Distributed, scalable time-series data storage.
	‚Ä¢	InfluxDB ‚Üí Optimized for telemetry & time-series GPU logs.
	‚Ä¢	Google BigQuery / Snowflake ‚Üí Cloud-based AI processing for analytics.

#### 6Ô∏è‚É£ How to Manage Data from Different Climate Zones (Hot vs. Cold Regions)?

Since GPUs are deployed in hot zones (Middle East, India, Africa) & cold zones (Canada, Russia, Scandinavia), we must adapt cooling strategies dynamically.

#### ‚úÖ Challenges:
	‚Ä¢	In hot regions (Saudi Arabia, UAE) ‚Üí Liquid cooling must run at max efficiency to prevent overheating.
	‚Ä¢	In cold regions (Canada, Finland) ‚Üí GPUs can use external ambient air cooling to reduce power usage.
	‚Ä¢	Humidity Control in High Moisture Areas ‚Üí Prevent condensation & electrical damage.

#### ‚úÖ AI-Powered Cooling Optimization Strategy:
	1.	Adaptive Cooling Mode ‚Üí AI models adjust cooling based on local weather conditions.
	2.	Dynamic Fan Speeds & Coolant Flow ‚Üí In hot regions, coolant flow increases dynamically.
	3.	Night Mode Power Saving ‚Üí In cold climates, GPUs reduce cooling usage during night cycles.
	4.	AI Forecasting for Heat Load ‚Üí Predicts thermal load based on workload trends & external weather.

 #### ‚úÖ Where Will We Send & Manage All Data?
	‚Ä¢	Central Cloud AI Hub ‚Üí Processes real-time GPU data from all locations.
	‚Ä¢	Regional Data Center AI Nodes ‚Üí Handles immediate GPU monitoring & cooling adjustments.
	‚Ä¢	Single Global Dashboard ‚Üí Admin can see all data centers, power usage, and alerts.

#### 7Ô∏è‚É£ Best Global Architecture for AI-Powered GPU Monitoring & Cooling

Global AI Hub (Cloud ML) ‚¨Ö Regional AI Nodes ‚¨Ö Edge AI in Each Data Center ‚¨Ö Telemetry from GPUs/DLC Sensors

#### ‚úÖ Data Flow:
##### 1Ô∏è‚É£ Telemetry from GPUs & Cooling Systems ‚¨á
##### 2Ô∏è‚É£ Sent to Local Edge AI Nodes for preprocessing ‚¨á
##### 3Ô∏è‚É£ Forwarded to Regional AI Nodes for real-time decision making ‚¨á
##### 4Ô∏è‚É£ Processed at the Central AI Hub for deep learning & anomaly detection ‚¨á
##### 5Ô∏è‚É£ Global Admin Dashboard provides full visibility & AI-driven optimizations.

#### **Which Features and Variables data we can get from GPU SDK?**

The NVIDIA Management Library (NVML) is a C-based API that provides comprehensive monitoring and management capabilities for NVIDIA GPU devices. It offers access to a wide range of features and variables, enabling detailed insights into GPU performance and status.  Ôøº

##### Key Features and Variables Accessible via NVML:
###### 1.	Identification Information:
	‚Ä¢	Product Name: Retrieve the official product name of the GPU.
	‚Ä¢	Serial Number: Access the unique serial number assigned to the GPU.
	‚Ä¢	PCI Device IDs: Obtain PCI identifiers for hardware recognition.
	‚Ä¢	VBIOS Version: Get the version of the GPU‚Äôs Video BIOS.
######	2.	Utilization Metrics:
	‚Ä¢	GPU Utilization: Monitor the percentage of GPU compute resources in use.
	‚Ä¢	Memory Utilization: Check the usage level of the GPU‚Äôs memory.
###### 3.	Memory Information:
	‚Ä¢	Total Memory: Determine the total available memory on the GPU.
	‚Ä¢	Used Memory: Find out how much memory is currently in use.
	‚Ä¢	Free Memory: Identify the amount of unused memory remaining.
###### 4.	Temperature and Fan Data:
	‚Ä¢	Core Temperature: Read the current temperature of the GPU core.
	‚Ä¢	Fan Speed: Access the speed of the GPU‚Äôs cooling fan (applicable to non-passive cooling solutions).
###### 5.	Power Management:
	‚Ä¢	Power Consumption: Measure the current power draw of the GPU.
	‚Ä¢	Power Limits: Retrieve or set the power management limits for the GPU.
###### 6.	Clock Information:
	‚Ä¢	Current Clock Speeds: Get the operating frequencies for various GPU components.
	‚Ä¢	Max Clock Speeds: Access the maximum supported frequencies.
	‚Ä¢	Performance State (PState): Determine the current performance state of the GPU.
###### 7.	Error Tracking:
	‚Ä¢	ECC Error Counts: Monitor both correctable and uncorrectable memory errors (for GPUs with ECC support).
	‚Ä¢	Error Logs: Access logs detailing various GPU errors.
###### 8.	Process Monitoring:
	‚Ä¢	Active Processes: List processes currently utilizing the GPU, including process IDs and allocated memory.
###### 9.	Compute Mode Management:
	‚Ä¢	Compute Mode Settings: Configure the GPU‚Äôs compute mode to control process access and execution behavior.
###### 10.	Persistence Mode:
	‚Ä¢	Persistence Mode Control: Enable or disable persistence mode, which keeps the GPU driver loaded even when no applications are using the GPU.

These features make NVML a powerful tool for developers and system administrators aiming to monitor and manage NVIDIA GPUs effectively. For detailed information and API references, consult the NVML API Reference Guide.

# Implementation

#### Step 1: Retrieve GPU Data using NVML (NVIDIA Management Library)

 To extract real-time GPU telemetry data (temperature, utilization, power, memory, etc.), we‚Äôll use NVML (NVIDIA Management Library) via the pynvml Python package.

 #### üìå Install Required Libraries

First, install pynvml for GPU monitoring:
"""

#  pip install nvidia-ml-py3

import pynvml
import time
import pandas as pd

#!apt-get update
#!apt-get install -y --no-install-recommends nvidia-driver-535''

#!apt install -y nvidia-utils-535

#   !apt-get install -y initramfs-tools

#!update-initramfs -u

# !echo "blacklist nouveau" >> /etc/modprobe.d/blacklist-nouveau.conf
# !echo "options nouveau modeset=0" >> /etc/modprobe.d/blacklist-nouveau.conf
# !update-initramfs -u

# import os
# os.environ["LD_LIBRARY_PATH"] = "/usr/lib/nvidia-535"

# üìå Initialize NVML
pynvml.nvmlInit()

# Get number of GPUs available
gpu_count = pynvml.nvmlDeviceGetCount()

print("gpu_count: ", gpu_count)

#!nvidia-smi

"""#### üìå Step 2: Code to Retrieve GPU Data in Real Time"""

import pynvml
import time
import pandas as pd

# üìå Initialize NVML
pynvml.nvmlInit()

# Get number of GPUs available
gpu_count = pynvml.nvmlDeviceGetCount()

# üìå Function to Retrieve GPU Data
def get_gpu_data():
    gpu_data_list = []

    for i in range(gpu_count):
        handle = pynvml.nvmlDeviceGetHandleByIndex(i)

        # Retrieve GPU metrics
        #gpu_name = pynvml.nvmlDeviceGetName(handle).decode("utf-8")
        gpu_name = pynvml.nvmlDeviceGetName(handle)
        gpu_utilization = pynvml.nvmlDeviceGetUtilizationRates(handle).gpu
        memory_utilization = pynvml.nvmlDeviceGetUtilizationRates(handle).memory
        total_memory = pynvml.nvmlDeviceGetMemoryInfo(handle).total / (1024**2)  # Convert to MB
        free_memory = pynvml.nvmlDeviceGetMemoryInfo(handle).free / (1024**2)
        used_memory = pynvml.nvmlDeviceGetMemoryInfo(handle).used / (1024**2)
        temperature = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)

        # Check if fan speed is supported before retrieving it
        try:
            fan_speed = pynvml.nvmlDeviceGetFanSpeed(handle)
        except pynvml.NVMLError_NotSupported:
            fan_speed = None  # Set to None if not supported

        power_usage = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000  # Convert mW to W
        max_power = pynvml.nvmlDeviceGetEnforcedPowerLimit(handle) / 1000  # Convert mW to W
        clock_speed = pynvml.nvmlDeviceGetClockInfo(handle, pynvml.NVML_CLOCK_GRAPHICS)

        # Store data in dictionary
        gpu_data = {
            "GPU Index": i,
            "GPU Name": gpu_name,
            "GPU Utilization (%)": gpu_utilization,
            "Memory Utilization (%)": memory_utilization,
            "Total Memory (MB)": total_memory,
            "Used Memory (MB)": used_memory,
            "Free Memory (MB)": free_memory,
            "Temperature (¬∞C)": temperature,
            "Fan Speed (%)": fan_speed,
            "Power Usage (W)": power_usage,
            "Max Power Limit (W)": max_power,
            "Clock Speed (MHz)": clock_speed
        }

        gpu_data_list.append(gpu_data)

    return pd.DataFrame(gpu_data_list)

# üìå Retrieve and Display GPU Data
gpu_df = get_gpu_data()
print(gpu_df)

"""#### üìå Step 3: Integrate Real-Time GPU Data into the LSTM Model

#### Full optimized code block
  1.	Checks if enough GPU telemetry data is available.
	2.	If missing, pads with the last known values (instead of zeros).
	3.	Reshapes it for LSTM inference.
	4.	Loads a pretrained LSTM model and makes predictions.
	5.	Converts predictions back to the original scale.
"""

from sklearn.preprocessing import MinMaxScaler
import numpy as np
import pandas as pd
from tensorflow.keras.models import load_model
import os

# Select relevant features
features = [
    "GPU Utilization (%)", "Memory Utilization (%)", "Temperature (¬∞C)",
    "Fan Speed (%)", "Power Usage (W)", "Clock Speed (MHz)"
]

# Normalize Data for LSTM
scaler = MinMaxScaler()

# Ensure `gpu_df` is available and has the required features
if "gpu_df" not in locals():
    raise ValueError("Error: `gpu_df` is not defined. Ensure you have collected GPU data.")

if not all(feature in gpu_df.columns for feature in features):
    raise ValueError("Error: `gpu_df` is missing one or more required features.")

# Normalize GPU data
gpu_df_scaled = scaler.fit_transform(gpu_df[features])

# Define sequence length
SEQ_LENGTH = 24  # Use last 24 readings to predict the next energy usage

# Check if enough data is available
if len(gpu_df_scaled) >= SEQ_LENGTH:
    X_real_time = np.array(gpu_df_scaled[-SEQ_LENGTH:])  # Use last 24 readings
else:
    # If not enough data, pad using the last known values
    padding_size = SEQ_LENGTH - len(gpu_df_scaled)

    if len(gpu_df_scaled) > 0:
        padding = np.tile(gpu_df_scaled[-1], (padding_size, 1))  # Repeat last row
    else:
        padding = np.zeros((padding_size, len(features)))  # Use zeros if no data

    X_real_time = np.concatenate((padding, gpu_df_scaled))  # Concatenate padding and real data

# Reshape for LSTM input
X_real_time = X_real_time.reshape(1, SEQ_LENGTH, len(features))

# Load Pretrained LSTM Model (Ensure the model exists)
try:
    #model = load_model("gpu_lstm_model.h5")
    model = model
except Exception as e:
    raise ValueError("Error: Could not load LSTM model. Ensure 'gpu_lstm_model.h5' exists.") from e

# Predict Future GPU Energy Usage
predicted_energy = model.predict(X_real_time)

# Convert Prediction Back to Original Scale
predicted_energy_unscaled = scaler.inverse_transform(
    np.concatenate((gpu_df_scaled[-1, :-1].reshape(1, -1), predicted_energy), axis=1)
)[:, -1]

print(f"Predicted Next Hour GPU Energy Consumption: {predicted_energy_unscaled[0]:.2f} Watts")

"""#### üìå How This Works?

#### 1Ô∏è‚É£ Retrieve real-time GPU metrics via NVML (utilization, power, temperature, memory, etc.).
#### 2Ô∏è‚É£ Preprocess the data (normalize & reshape for LSTM model).
#### 3Ô∏è‚É£ Load the pre-trained LSTM model and make real-time energy consumption predictions.
#### 4Ô∏è‚É£ Convert the prediction back to real-world units for easy interpretation.
"""