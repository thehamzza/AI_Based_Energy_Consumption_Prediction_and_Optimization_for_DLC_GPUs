# -*- coding: utf-8 -*-
"""AI_Based_Energy_Consumption_Prediction_and_Optimization_for_DLC_GPUs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HgvtXVPUyP9AK1KUYCOdAm7ON-8Kx59V

# AI-Based Energy Consumption Prediction & Optimization for DLC GPUs

### Project by: Muhammad Hamza

#### Dated: 21 Feb, 2025

#### üåê Website: https://mhamza.site/
#### üì© Contact: https://mhamza.site/contact/

### 1. Introduction

Direct Liquid Cooling (DLC) is a cutting-edge cooling method used to manage the heat generated by high-performance GPUs. Optimizing cooling strategies is critical for reducing energy consumption, operational costs, and hardware degradation. Traditional cooling systems operate inefficiently by running at fixed rates, consuming unnecessary power during low workloads and failing to cool effectively during high-demand periods.

Artificial Intelligence (AI) provides a predictive and adaptive approach to cooling by forecasting energy consumption and dynamically adjusting cooling parameters. This report explores AI-based energy forecasting for DLC GPUs, compares different machine learning (ML) models, and presents a working Python implementation.

### 2. Problem Statement

#### Why Predict GPU Energy Consumption?
	‚Ä¢	Cooling costs contribute significantly to the total energy consumption in high-performance computing.
	‚Ä¢	Inefficient cooling increases power usage and reduces hardware lifespan.
	‚Ä¢	Predicting power consumption allows us to adjust cooling dynamically,
             preventing overheating and reducing operational costs.

### Key Objectives
	1.	Predict GPU power consumption based on historical data and environmental factors.
	2.	Optimize cooling system response dynamically.
	3.	Reduce energy costs by preventing unnecessary cooling power usage.
	4.	Extend hardware lifespan by minimizing thermal stress.

## 3. Background Concepts

### 3.1 Energy Consumption & Cooling Efficiency

#### GPU power usage is affected by:
	‚Ä¢	Workload (AI computations, gaming, rendering).
	‚Ä¢	Ambient temperature (higher temperatures lead to increased power draw).
	‚Ä¢	Cooling system efficiency (poor cooling leads to overheating and throttling).
	‚Ä¢	Humidity & airflow conditions.

### 3.2 Time-Series Forecasting

##### Predicting energy consumption is a time-series problem, as power usage varies over time. Common models used:
	‚Ä¢	ARIMA (traditional statistical model).
	‚Ä¢	Random Forest/XGBoost (tree-based ML models for structured data).
	‚Ä¢	LSTMs & Transformers (deep learning models for sequential data).

#### 3.3 Dynamic Cooling Control

##### AI can adjust cooling power dynamically based on energy predictions by:
	‚Ä¢	Increasing cooling before GPU overheats.
	‚Ä¢	Reducing cooling when power usage is low.
	‚Ä¢	Avoiding unnecessary cooling costs.

### 4. AI Solution Approach

#### 4.1 Data Collection

#### Two primary datasets:
	1.	GPU Power Consumption Logs ‚Äì Collected using tools like nvidia-smi or cloud-based monitoring.
   
	2.	Weather Data ‚Äì Fetched from free APIs like OpenWeatherMap and Meteostat.

    https://openweathermap.org/api
    https://dev.meteostat.net/

#### 4.2 Feature Engineering
	‚Ä¢	Time Features: Hour of the day, day of the week.
	‚Ä¢	Weather Features: Temperature, humidity, wind speed.
	‚Ä¢	GPU Metrics: Utilization, workload type.

#### 4.3 Model Selection

We compare three ML approaches:

![image.png](attachment:e657e209-8b70-4ab5-ac2f-62b06f781343.png)

#### 5. Python Implementation

5.1 Install Required Libraries
"""

print ("hello")

import pandas, numpy, sklearn, xgboost, requests, matplotlib
print("All libraries are installed!")

import sys
print(sys.path)

sys.path.append('/lib/python3.12/site-packages')  # Adjust for your Python version

# Commented out IPython magic to ensure Python compatibility.
# %pip install meteostat

# üìå Step 1: Import Required Libraries
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error
import matplotlib.pyplot as plt

print("All libraries are imported!")

"""#### I have generated a synthetic dataset for GPU energy consumption and weather conditions over a 10-day period with hourly data. The dataset includes:
	‚Ä¢	Temperature (¬∞C)
	‚Ä¢	Humidity (%)
	‚Ä¢	Wind Speed (km/h)
	‚Ä¢	GPU Power Consumption (Watts)

#### We can now use this dataset to train machine learning models for energy prediction and cooling optimization.

# Step 2: Generate Synthetic Weather and GPU Energy Data
"""

# Generate timestamps for 10 days (hourly data)
timestamps = pd.date_range(start="2025-01-01", periods=240, freq='h')

print("timestamps: ", timestamps)

# Generate synthetic weather data
temperature = np.random.uniform(0, 35, len(timestamps))  # Temperature in ¬∞C
humidity = np.random.uniform(20, 80, len(timestamps))  # Humidity in %
wind_speed = np.random.uniform(0, 20, len(timestamps))  # Wind Speed in km/h

print("temperature ¬∞C: ", temperature)

print("humidity %: ", humidity)

print("wind_speed km/h: ", wind_speed)

# Generate synthetic GPU power consumption (in watts)
gpu_energy = np.sin(np.linspace(0, 10, len(timestamps))) * 50 + 200 + np.random.uniform(-10, 10, len(timestamps))

print("gpu_energy W: ", gpu_energy)

# Create DataFrame
dummy_data = pd.DataFrame({
    'timestamp': timestamps,
    'temperature': temperature,
    'humidity': humidity,
    'wind_speed': wind_speed,
    'gpu_energy': gpu_energy
})

# Set timestamp as index
dummy_data.set_index('timestamp', inplace=True)

# Display the first few rows of the dataset
print("Synthetic Weather and GPU Energy Data (First 5 Rows):")
print(dummy_data.head())

"""# Step 3: Split Data for Machine Learning Models"""

# Selecting features and target variable
features = ['temperature', 'humidity', 'wind_speed']
X = dummy_data[features]
y = dummy_data['gpu_energy']

# Splitting dataset into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""# Step 4: Prepare Machine Learning Models"""

# Uncomment this section when you are ready to train the models
# Define machine learning models
models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(n_estimators=50, random_state=42),  # Reduced estimators for faster execution
    "XGBoost": XGBRegressor(n_estimators=10, random_state=42)  # Reduced boosting rounds for efficiency
}

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Train models and evaluate performance
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)  # Train the model
    y_pred = model.predict(X_test)  # Make predictions
    mae = mean_absolute_error(y_test, y_pred)  # Compute Mean Absolute Error (MAE)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))  # Compute RMSE
    r2 = r2_score(y_test, y_pred)  # Compute R¬≤ Score
    results[name] = {"MAE": mae, "RMSE": rmse, "R2 Score": r2, "Predictions": y_pred}

# Convert results into a DataFrame
performance_df = pd.DataFrame([{**{"Model": k}, **v} for k, v in results.items()]).drop(columns=["Predictions"])

print("performance_df: ", performance_df)

"""# Step 5: Visualize Model Performance"""

# Bar Chart for Mean Absolute Error (MAE)
plt.figure(figsize=(10, 5))
plt.bar(performance_df["Model"], performance_df["MAE"], color=['blue', 'green', 'red'])
plt.xlabel("Machine Learning Models")
plt.ylabel("Mean Absolute Error (Watts)")
plt.title("Model Performance Comparison (Lower MAE is Better)")
plt.grid(axis='y')
# Save the figure in the root directory
plt.savefig("bar_chart_for_mean_absolute_error.png")

#plt.show()
# Close the figure to prevent it from blocking execution
#plt.close("all")

# Print confirmation message
print("Plot saved as 'bar_chart_for_mean_absolute_error.png' in the root folder.")

"""### Conclusion:

#### Linear Regression (BEST)
	‚Ä¢	MAE: 28.86 | RMSE: 32.99 | R¬≤ Score: 0.0169
	‚Ä¢	Assumes a linear relationship between power consumption and weather variables.
	‚Ä¢	Struggles to capture complex dependencies, leading to higher errors.
	‚Ä¢	Works best as a baseline model, but lacks real-world applicability.

#### Random Forest
	‚Ä¢	MAE: 30.00 | RMSE: 35.47 | R¬≤ Score: -0.1365
	‚Ä¢	Handles non-linearity better than Linear Regression.
	‚Ä¢	Expected to perform better, but higher errors indicate possible overfitting.
	‚Ä¢	Not optimized for time-series dependencies, limiting its forecasting accuracy.
    
#### XGBoost:
	‚Ä¢	MAE: 29.43 | RMSE: 34.69 | R¬≤ Score: -0.0872
	‚Ä¢	Uses gradient boosting for iterative improvements.
	‚Ä¢	Expected to perform well, but underperformed due to hyperparameter limitations.
	‚Ä¢	Requires parameter tuning and a larger dataset for better generalization.
"""

# Display Model Performance Metrics
print("\nüìå Model Performance Metrics:")
print(performance_df)
print("I am here 3")

"""### Key Takeaways:

#### ‚úÖ Linear Regression performed the best in terms of R¬≤ Score (0.0169), but still had relatively high errors.
#### ‚úÖ Random Forest surprisingly underperformed, with the highest MAE (30.00) and RMSE (35.47), suggesting overfitting.
#### ‚úÖ XGBoost had negative R¬≤ (-0.0872), meaning it did worse than a simple average prediction, requiring hyperparameter tuning.


### Observations from the Table:
#### Mean Absolute Error (MAE) ‚Üí Measures average absolute differences between actual and predicted values.
	‚Ä¢	Lowest: Linear Regression (28.86) ‚Üí Performs slightly better in absolute error reduction.
	‚Ä¢	Highest: Random Forest (30.00) ‚Üí Indicates higher prediction inconsistencies.
#### Root Mean Squared Error (RMSE) ‚Üí Penalizes large errors more than MAE.
	‚Ä¢	Lowest: Linear Regression (32.99) ‚Üí Handles large fluctuations better.
	‚Ä¢	Highest: Random Forest (35.47) ‚Üí Suggests larger prediction errors.
#### R¬≤ Score (Coefficient of Determination) ‚Üí Measures how well the model explains variability (closer to 1 is better).
	‚Ä¢	Highest: Linear Regression (0.0169) ‚Üí Explains very little variance but still better than others.
	‚Ä¢	Lowest: XGBoost (-0.0872) ‚Üí Negative value means the model performs worse than a basic mean predictor.

### How to Improve Model Performance?

#### ‚úÖ Feature Engineering
	‚Ä¢	Introduce time-based features (hour of the day, weekday vs. weekend).
	‚Ä¢	Incorporate GPU workload/utilization metrics to improve accuracy.

#### ‚úÖ Hyperparameter Tuning
	‚Ä¢	Optimize XGBoost parameters (learning rate, estimators, max depth).
	‚Ä¢	Perform Grid Search or Bayesian Optimization for fine-tuning.

#### ‚úÖ Switch to Time-Series Models
	‚Ä¢	Instead of Random Forest and XGBoost, try LSTM or Transformer models to better capture sequential energy consumption patterns.

#### ‚úÖ Use Real-World Data
	‚Ä¢	Fetch live weather data from OpenWeatherMap API to improve prediction accuracy.
	‚Ä¢	Train on a larger dataset to help models generalize better.

# Step 6: Visualizing the Data
"""

# Plot GPU energy consumption over time
plt.figure(figsize=(12, 5))
plt.plot(dummy_data.index, dummy_data['gpu_energy'], label="GPU Energy Consumption (Watts)", color='blue')
plt.xlabel("Timestamp")
plt.ylabel("Power Consumption (Watts)")
plt.title("Synthetic GPU Energy Consumption Over Time")
plt.legend()
plt.grid()
#plt.show()
#plt.show(block=False)

"""# Step 7: Future Predictions (Using Best Model)"""

# Generate synthetic future weather data for prediction
future_timestamps = pd.date_range(start="2025-03-01", periods=24, freq='h')
future_weather = pd.DataFrame({
    "temperature": np.random.uniform(0, 35, len(future_timestamps)),
    "humidity": np.random.uniform(20, 80, len(future_timestamps)),
    "wind_speed": np.random.uniform(0, 20, len(future_timestamps))
}, index=future_timestamps)

future_weather.head()

# Predict future GPU power consumption using the best model
# best_model = models["Random Forest"]
best_model = models["Linear Regression"]
future_predictions = best_model.predict(future_weather)

# Create DataFrame for future predictions
future_df = pd.DataFrame({"timestamp": future_timestamps, "Predicted GPU Energy": future_predictions})
future_df.set_index("timestamp", inplace=True)

"""# Step 8: Visualize Future Predictions"""

# Step 8: Visualize Future Predictions

plt.figure(figsize=(12, 5))
plt.plot(future_df.index, future_df["Predicted GPU Energy"], label="Predicted GPU Energy (Watts)", color='purple', linestyle='--')
plt.xlabel("Timestamp")
plt.ylabel("Predicted Power Consumption (Watts)")
plt.title("Future Predictions of GPU Energy Consumption")
plt.legend()
plt.grid()
plt.show()

# Display Future Predictions
print("\nüìå Future GPU Energy Predictions:")
print(future_df)

"""# Future Integration with Real Data
This implementation currently uses **synthetic data**, but real-time weather data can be used by fetching it from APIs such as:
- OpenWeatherMap: https://openweathermap.org/api
- Meteostat: https://dev.meteostat.net/

By replacing the dummy dataset with real API-fetched weather data, we can enhance the model's predictive accuracy in practical applications.

#### Features of This Code

##### ‚úÖ Generates synthetic weather & GPU power data.
##### ‚úÖ Trains multiple machine learning models (Linear Regression, Random Forest, XGBoost).
##### ‚úÖ Compares performance using MAE, RMSE, and R¬≤ Score.
##### ‚úÖ Displays model performance in a bar chart.
##### ‚úÖ Predicts and visualizes GPU energy consumption for the next 24 hours.
##### ‚úÖ Future integration possible with real-time data APIs.

### 7. Implementation of AI-Based Cooling

#### 7.1 How to Apply AI in Cooling Optimization?

##### ‚úÖ If AI predicts high energy consumption: Increase cooling preemptively.
##### ‚úÖ If AI predicts low energy consumption: Reduce cooling power to save costs.
##### ‚úÖ If peak pricing is detected: Schedule heavy workloads for off-peak hours.

#### 7.2 Integrating AI with Cooling Systems
	‚Ä¢	Connect AI to fan controllers and liquid cooling pumps.
	‚Ä¢	Use Reinforcement Learning (RL) to optimize cooling dynamically.
	‚Ä¢	Deploy AI model in real-time monitoring systems.

### 8. Conclusion

##### ‚úÖ AI-powered energy forecasting reduces cooling costs.
##### ‚úÖ Machine learning models (XGBoost) outperform simple models.
##### ‚úÖ Predictive cooling extends GPU lifespan and avoids peak pricing.
##### ‚úÖ Future improvements: Deep Learning (LSTMs), cloud deployment, real-world integration.

### 9. Next Steps
	‚Ä¢	Collect real-world GPU logs to replace synthetic data.
	‚Ä¢	Implement LSTM models for long-term forecasting.
	‚Ä¢	Deploy model as an API to control cooling in real-time.

##**Testing LSTM Model**
24-Feb-2025
"""

# üìå Step 1: Import Required Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.model_selection import train_test_split
import joblib

# üìå Step 2: Generate Synthetic Data (Realistic Variations & Anomalies)
np.random.seed(42)

timestamps = pd.date_range(start="2024-01-01", periods=1440, freq='h')

print("timestamps: ", timestamps)

# Simulated weather patterns
temperature = 15 + 10 * np.sin(np.linspace(0, 6.28, len(timestamps))) + np.random.normal(0, 2, len(timestamps))
humidity = 50 + 10 * np.cos(np.linspace(0, 6.28, len(timestamps))) + np.random.normal(0, 5, len(timestamps))
wind_speed = np.random.uniform(5, 25, len(timestamps))
air_pressure = np.random.uniform(990, 1025, len(timestamps))
dew_point = temperature - np.random.uniform(2, 5, len(timestamps))
cloud_cover = np.abs(50 * np.sin(np.linspace(0, 6.28, len(timestamps)))) + np.random.normal(0, 10, len(timestamps))

# GPU-related features
gpu_utilization = np.clip(40 + 30 * np.sin(np.linspace(0, 12.56, len(timestamps))) + np.random.normal(0, 10, len(timestamps)), 0, 100)
fan_speed = np.clip(2000 + 500 * np.sin(np.linspace(0, 12.56, len(timestamps))) + np.random.normal(0, 200, len(timestamps)), 1000, 4000)
core_temperature = np.clip(50 + 10 * np.sin(np.linspace(0, 12.56, len(timestamps))) + np.random.normal(0, 5, len(timestamps)), 30, 90)
vram_utilization = np.clip(4000 + 1000 * np.sin(np.linspace(0, 6.28, len(timestamps))) + np.random.normal(0, 500, len(timestamps)), 1000, 8000)
power_limit = np.random.uniform(80, 120, len(timestamps))

# Workload Types (Categorical Encoding)
workload_encoded = np.random.choice([0, 1, 2, 3, 4], len(timestamps), p=[0.3, 0.3, 0.2, 0.1, 0.1])

# Time-based features
hour_of_day = timestamps.hour
day_of_week = timestamps.dayofweek
month_of_year = timestamps.month

# Simulating GPU Energy Consumption (Target Variable)
gpu_energy = ((150 + (gpu_utilization * 1.8) + (temperature * 0.6) - (fan_speed * 0.002)) + ((core_temperature * 0.9) + np.random.normal(0, 5, len(timestamps))))

# Injecting Anomalies Every ~200 Hours
for i in range(0, len(gpu_energy), 200):
    gpu_energy[i] += np.random.uniform(50, 100)

# Create DataFrame
df = pd.DataFrame({
    'timestamp': timestamps,
    'temperature': temperature,
    'humidity': humidity,
    'wind_speed': wind_speed,
    'air_pressure': air_pressure,
    'dew_point': dew_point,
    'cloud_cover': cloud_cover,
    'gpu_utilization': gpu_utilization,
    'fan_speed': fan_speed,
    'core_temperature': core_temperature,
    'vram_utilization': vram_utilization,
    'power_limit': power_limit,
    'workload_type': workload_encoded,
    'hour_of_day': hour_of_day,
    'day_of_week': day_of_week,
    'month_of_year': month_of_year,
    'gpu_energy': gpu_energy
})

df.set_index('timestamp', inplace=True)

print("df: ", df.head())

# üìå Step 3: Preprocess Data (Normalization & Sequence Formatting)
features = df.columns[:-1]
target = 'gpu_energy'

scaler_features = MinMaxScaler()
scaler_target = MinMaxScaler()

df_scaled = df.copy()
df_scaled[features] = scaler_features.fit_transform(df[features])
df_scaled[target] = scaler_target.fit_transform(df[[target]])

data_array = df_scaled[features.tolist() + [target]].values
# Convert features to a list before concatenation

SEQ_LENGTH = 24  # Use last 24 hours to predict next-hour GPU energy

def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i+seq_length, :-1])
        y.append(data[i+seq_length, -1])
    return np.array(X), np.array(y)

X, y = create_sequences(data_array, SEQ_LENGTH)

# üìå Step 4: Split Data for Training & Testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)

# üìå Step 5: Build & Train LSTM Model
model = Sequential([
    LSTM(64, return_sequences=True, input_shape=(SEQ_LENGTH, X.shape[2])),
    Dropout(0.2),
    LSTM(64, return_sequences=False),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1)
])

model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data=(X_test, y_test), verbose=1)


# Saving the model

# Save the model & scaler
model.save("gpu_energy_model.h5")
joblib.dump(scaler, "scaler.pkl")

print("Model training complete and saved!")


# üìå Step 6: Evaluate Model Performance
y_pred = model.predict(X_test)

# Inverse scaling
y_pred_unscaled = scaler_target.inverse_transform(y_pred.reshape(-1, 1)).flatten()
y_test_unscaled = scaler_target.inverse_transform(y_test.reshape(-1, 1)).flatten()

# Performance Metrics
mae = mean_absolute_error(y_test_unscaled, y_pred_unscaled)
rmse = np.sqrt(mean_squared_error(y_test_unscaled, y_pred_unscaled))
r2 = r2_score(y_test_unscaled, y_pred_unscaled)

print(f"üìå LSTM Model Performance:")
print(f"MAE: {mae:.2f} Watts")
print(f"RMSE: {rmse:.2f} Watts")
print(f"R¬≤ Score: {r2:.4f}")

errors = np.abs(y_test_unscaled - y_pred_unscaled)
threshold = np.percentile(errors, 95)
anomalies = errors > threshold

anomaly_df = pd.DataFrame({
    "Timestamp": df.index[-len(y_test_unscaled):],
    "Actual GPU Energy": y_test_unscaled,
    "Predicted GPU Energy": y_pred_unscaled,
    "Error": errors,
    "Anomaly": anomalies
})

print("\nüìå Detected Anomalies in GPU Energy Consumption:")
print(anomaly_df[anomaly_df["Anomaly"] == True])

# üìå Step 8: Visualization
plt.figure(figsize=(12, 5))
plt.plot(df.index[-len(y_test_unscaled):], y_test_unscaled, label="Actual GPU Energy", color='blue')
plt.plot(df.index[-len(y_test_unscaled):], y_pred_unscaled, label="Predicted GPU Energy", color='red', linestyle='dashed')
plt.scatter(anomaly_df["Timestamp"][anomaly_df["Anomaly"]], anomaly_df["Actual GPU Energy"][anomaly_df["Anomaly"]], color='orange', label="Detected Anomalies", marker='o')
plt.xlabel("Timestamp")
plt.ylabel("GPU Energy Consumption (Watts)")
plt.title("LSTM Model: GPU Energy Consumption Prediction & Anomalies")
plt.legend()
plt.grid()
plt.show()

print("Program Executed Successfully.")